{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read initial chunk from 'IAKR7EFL.DTA'.\n",
      "An error occurred while processing 'IAMR7EFL.DTA': \"None of [Index(['caseid', 'v001', 'v002', 'v003', 'age', 'daily_calorie_intake',\\n       'meals_per_day', 'platelet_count_thousands', 'triglycerides_mg_dl',\\n       'urinary_arsenic_ug_l', 'weight_kg', 'respiratory_symptoms',\\n       'immunization_status', 'tsh_uIU_ml (lab)',\\n       'urinary_potassium_mmol_l (lab)', 'transferrin_mg_dl (lab)',\\n       'urinary_sodium_mmol_l (lab)', 'serum_albumin_g_dl (lab)',\\n       'tsh_uIU_ml (lab)', 'urinary_potassium_mmol_l (lab)',\\n       'transferrin_mg_dl (lab)', 'urinary_sodium_mmol_l (lab)',\\n       'serum_albumin_g_dl (lab)', 'tsh_uIU_ml (lab)',\\n       'urinary_potassium_mmol_l (lab)', 'transferrin_mg_dl (lab)',\\n       'urinary_sodium_mmol_l (lab)', 'serum_albumin_g_dl (lab)',\\n       'tsh_uIU_ml (lab)', 'urinary_potassium_mmol_l (lab)',\\n       'transferrin_mg_dl (lab)', 'urinary_sodium_mmol_l (lab)',\\n       'serum_albumin_g_dl (lab)', 'tsh_uIU_ml (lab)',\\n       'urinary_potassium_mmol_l (lab)', 'transferrin_mg_dl (lab)',\\n       'urinary_sodium_mmol_l (lab)', 'serum_albumin_g_dl (lab)',\\n       'tsh_uIU_ml (lab)', 'urinary_potassium_mmol_l (lab)',\\n       'transferrin_mg_dl (lab)', 'urinary_sodium_mmol_l (lab)',\\n       'serum_albumin_g_dl (lab)', 'tsh_uIU_ml (lab)',\\n       'urinary_potassium_mmol_l (lab)', 'transferrin_mg_dl (lab)',\\n       'urinary_sodium_mmol_l (lab)', 'serum_albumin_g_dl (lab)',\\n       'tsh_uIU_ml (lab)', 'urinary_potassium_mmol_l (lab)',\\n       'transferrin_mg_dl (lab)', 'urinary_sodium_mmol_l (lab)',\\n       'serum_albumin_g_dl (lab)', 'tsh_uIU_ml (lab)',\\n       'urinary_potassium_mmol_l (lab)'],\\n      dtype='object')] are in the [columns]\"\n",
      "An error occurred while processing 'IAIR7EFL.DTA': '[\\'marital_status\\', \\'occupation\\', \\'fuel_collection_time\\', \\'commute_time\\', \\'fat_grams\\', \\'hemoglobin_g_dl\\', \\'hba1c_pct\\', \\'vitaminD_ng_ml\\', \\'urine_albumin_mg_g\\', \\'waist_cm\\', \\'chronic_bronchitis\\', \\'alcohol_use\\', \"Women\\'s - menstrual_regular\"] not in index'\n",
      "An error occurred while processing 'IAHR7EFL.DTA': \"None of [Index(['caseid', 'v001', 'v002', 'v003', 'education_years',\\n       'employment_status', 'iron_mg', 'hematocrit_pct',\\n       'total_cholesterol_mg_dl', 'crp_mg_l', 'creatinine_mg_dl', 'hip_cm',\\n       'diabetes_diagnosis', 'physical_activity_min_week',\\n       'awareness_air_pollution', 'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score',\\n       'Elderly - activities_of_daily_living_score'],\\n      dtype='object')] are in the [columns]\"\n",
      "An error occurred while processing 'IACR7EFL.DTA': \"['literacy', 'vitaminA_ug', 'rbc_count_million', 'ldl_mg_dl', 'blood_lead_ug_dl', 'eGFR_ml_min_1_73m2', 'blood_pressure_sys', 'hypertension_diagnosis', 'clean_fuel_attitude', 'phq9_score score'] not in index\"\n",
      "An error occurred while processing 'IABR7EFL.DTA': \"['diet_diversity_score', 'wbc_count_thousands', 'hdl_mg_dl', 'blood_cadmium_ug_l', 'height_cm', 'blood_pressure_dia', 'self_rated_health', 'health_insurance', 'vitamin_b12 (dietary estimate)', 'protein (dietary estimate)'] not in index\"\n",
      "\n",
      "✅ Successfully created merged file at 'merged_household_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_dta_files(json_path, output_excel_path, data_folder_path, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Reads a JSON file for merging instructions, processes DTA files in chunks,\n",
    "    merges them based on person and household IDs, and saves the result to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The file path for the JSON file with merging instructions.\n",
    "        output_excel_path (str): The file path for the output Excel file.\n",
    "        data_folder_path (str): The path to the folder containing the DTA files.\n",
    "        chunk_size (int): The number of rows to process from the initial DTA file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSON file not found at '{json_path}'\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{json_path}'\")\n",
    "        return\n",
    "\n",
    "    # Define merging keys from the JSON file\n",
    "    person_keys = ['caseid', 'v001', 'v002', 'v003']\n",
    "    household_keys = ['hhid', 'v001', 'v002'] \n",
    "\n",
    "\n",
    "    # Group features by their source file for efficient processing\n",
    "    features_by_file = defaultdict(lambda: {'person': [], 'household': []})\n",
    "    for feature in config.get('useful', []):\n",
    "        source_file = feature.get('source_file')\n",
    "        feature_name = feature.get('feature_name')\n",
    "        merge_on = feature.get('merge_on')\n",
    "\n",
    "        if source_file and feature_name:\n",
    "            if merge_on == 'person_id':\n",
    "                features_by_file[source_file]['person'].append(feature_name)\n",
    "            elif merge_on == 'household_id':\n",
    "                features_by_file[source_file]['household'].append(feature_name)\n",
    "\n",
    "    # Identify the primary file to start the merge process\n",
    "    # This is typically the file with the most person-level features\n",
    "    primary_file = max(features_by_file, key=lambda f: len(features_by_file[f]['person']))\n",
    "    \n",
    "    # Start with the first chunk of the primary file\n",
    "    try:\n",
    "        reader = pd.read_stata(f\"{data_folder_path}/{primary_file}\", chunksize=chunk_size, convert_categoricals=False)\n",
    "        merged_df = next(reader)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Primary DTA file not found at '{data_folder_path}/{primary_file}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading '{primary_file}': {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Successfully read initial chunk from '{primary_file}'.\")\n",
    "\n",
    "    # Sequentially merge features from each file\n",
    "    for file, features in features_by_file.items():\n",
    "        if file == primary_file:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read the corresponding chunk from the other files\n",
    "            other_reader = pd.read_stata(f\"{data_folder_path}/{file}\", chunksize=chunk_size, convert_categoricals=False)\n",
    "            other_df = next(other_reader)\n",
    "\n",
    "            # Merge person-level features\n",
    "            if features['person']:\n",
    "                cols_to_merge = person_keys + features['person']\n",
    "                merged_df = pd.merge(merged_df, other_df[cols_to_merge], on=person_keys, how='left')\n",
    "                print(f\"Merged person features from '{file}'.\")\n",
    "\n",
    "            # Merge household-level features\n",
    "            if features['household']:\n",
    "                # Ensure household keys in the file are correctly identified\n",
    "                current_household_keys = [k for k in household_keys if k in other_df.columns]\n",
    "                cols_to_merge = current_household_keys + features['household']\n",
    "                \n",
    "                # Check if all household keys are present for merging\n",
    "                if all(k in merged_df.columns for k in current_household_keys):\n",
    "                    merged_df = pd.merge(merged_df, other_df[cols_to_merge], on=current_household_keys, how='left')\n",
    "                    print(f\"Merged household features from '{file}'.\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not merge household data from '{file}' due to missing keys.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: DTA file not found at '{data_folder_path}/{file}'. Skipping.\")\n",
    "        except StopIteration:\n",
    "            print(f\"Warning: No more data in '{file}' to merge. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{file}': {e}\")\n",
    "\n",
    "    # Save the final merged dataframe to an Excel file\n",
    "    try:\n",
    "        merged_df.to_excel(output_excel_path, index=False)\n",
    "        print(f\"\\n✅ Successfully created merged file at '{output_excel_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the Excel file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Path to the JSON file with merging instructions\n",
    "    JSON_FILE_PATH = 'household_features_400_200.json'\n",
    "    \n",
    "    # Path for the final merged Excel file\n",
    "    OUTPUT_EXCEL_PATH = 'merged_household_data.xlsx'\n",
    "    \n",
    "    # Folder containing all the DTA files\n",
    "    # Create a folder named 'Dataset' and place all your .dta files inside it.\n",
    "    DATA_FOLDER_PATH = 'Dataset' \n",
    "    \n",
    "    # --- Execution ---\n",
    "    merge_dta_files(JSON_FILE_PATH, OUTPUT_EXCEL_PATH, DATA_FOLDER_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
